{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader,PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db_aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mohan-experience\\\\resume']\n",
      "Total number of chunks: 11\n",
      "Document types found: {'resume'}\n"
     ]
    }
   ],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"Mohan-experience/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "documents = []\n",
    "print(folders)\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    #print(loader)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 11 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Time to use LangChain to bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "# workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# def call_model(state: MessagesState):\n",
    "#     response = model.invoke(state[\"messages\"])\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": response}\n",
    "\n",
    "\n",
    "# # Define the two nodes we will cycle between\n",
    "# workflow.add_edge(START, \"model\")\n",
    "# workflow.add_node(\"model\", call_model)\n",
    "# memory = MemorySaver()\n",
    "\n",
    "# app = workflow.compile(\n",
    "#     checkpointer=memory\n",
    "# )\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://41bb5a7664ce37166d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://41bb5a7664ce37166d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55e9abb-e1da-46c5-acba-911868aee329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Mohan Salal\n",
      "♂¶obile-alt+91-8937930647, 8104941883 • /envel⌢pemohansalal14296@gmail.com\n",
      "/linkedin-inmohan-salal-861222a7\n",
      "Skills\n",
      "Languages: Java, JavaScript, React, Groovy, Python, SQL, NoSQL, HTML, CSS\n",
      "Frameworks: Java Spring, JUnit\n",
      "Cloud & DevOps : AWS, Kubernetes, Docker\n",
      "Other: SAP Commerce, Solr, Swagger, GraphQL, Kafka, Microservices, Webservices, SAP Punchout,\n",
      "cXML, GenAI, Git, Cassandra, PostgreSQL, MySQL\n",
      "Professional Experience\n",
      "Deloitte, Gurugram, Haryana\n",
      "Consultant Sep 2021 – Present\n",
      "○ Worked on an order data transfer solution to Azure Topic, implemented an efficient retry mechanism to automatically\n",
      "reprocess failed records after a set interval.\n",
      "○ Implemented REST APIs for retrieving order data and updating the order delivery in commerce system from 3rd\n",
      "party.\n",
      "○ Worked on the SAP PunchOut project with procurement systems such as Ariba, Jaggaer, Coupa, and Oracle.\n",
      "Implemented logic for invoice cXML transmission to all the procurement system.\n",
      "○ Worked on cart and checkout flow that utilized customer behavior analytics to optimize checkout journey, leading\n",
      "to a 15% increase in conversion rates and enhancing revenue generation for the platform.\n",
      "○ Developed and integrated a robust backend system for order processing that enabled seamless data transfer to\n",
      "S/4 HANA. This ensured 100% data accuracy across systems.\n",
      "○ Created a high-performance product search solution using Solr, improved discoverability by 30%.\n",
      "○ Engineered a customized promotion named “Gift with Choice” within the e-commerce application, facilitating\n",
      "user-driven gift selection. This feature is now utilized by thousands of customers, enhancing overall shopping\n",
      "experience and satisfaction.\n",
      "○ Developed microservice for stock reservation and improved system resilience and fault tolerance by using circuit\n",
      "breaker pattern.\n",
      "○ Developed groovy scripts to automate the migration of over 500,000 historical transactional records to new\n",
      "systems from Legacy system, achieving a 40% reduction in manual processing time.\n",
      "○ Resolved critical bugs and served as primary/secondary on-call support, monitoring and addressing production\n",
      "issues.\n",
      "○ Working closely with the product owner to translate complex business requirements into technical solutions.\n",
      "Nagarro, Gurugram, Haryana\n",
      "Senior Software Engineer Jun 2020 – Sep 2021\n",
      "○ Identified and rectified 15+ major issues related to web services and promotional tools, leading to a 20% increase\n",
      "in successful order completions and enhancing overall system reliability for clients.\n",
      "○ Developed a stock reservation logic, reducing cart abandonment by 25%.\n",
      "○ Engineered an automated payment processing system for subscription-based products, it has increased customer\n",
      "satisfaction scores by 15% within the first quarter of implementation.\n",
      "○ Served as primary/secondary On-call support, responsible for monitoring and fixing production issues if something\n",
      "breaks\n",
      "Accenture, Gurugram, Haryana\n",
      "Software Engineer Apr 2019 – Jun 2020\n",
      "1/2\n",
      "\n",
      "Accenture, Gurugram, Haryana Apr 2019 – Jun 2020 \n",
      "Software Engineer \n",
      "● Engaged with clients through 12 structured meetings to extract detailed requirements for the \n",
      "Order Reconciliation project, the resulting insights directly influenced the design phase, \n",
      "ensuring alignment with client expectations. \n",
      "● Spearheaded the design and development of a comprehensive Order Reconciliation system \n",
      "through SAP Commerce, incorporating composite cronjobs that reduced order discrepancies \n",
      "by 40% and improved processing time by 25%. \n",
      "● Analyzed error logs and identified root causes for recurring production failures, implemented \n",
      "target fixes that decreased error rates by 45% and improved system reliability for over 30000 \n",
      "daily active users. \n",
      " \n",
      " \n",
      "TCS, Mumbai Feb 2017 – Apr 2019 \n",
      "Assistant Systems Engineer \n",
      "● Designed a machine learning framework for OCR that analyzed and recognized text from diverse \n",
      "formats, increasing recognition accuracy by 60% and reducing error rates in data entry, leading to \n",
      "improved overall productivity. \n",
      "● Developed the customize UI and components for Croma (PLP, Cart Page, Rotating Banners, \n",
      "Home Page, Headers, Shop-By- Category, PDP tech specification, 3 party integration for rating \n",
      "and reviews data, Customized rating and review for Croma, Compare page and other static \n",
      "pages). \n",
      "● Implemented a streamlined PIM reporting Cron Job that reduced data processing time by 60% \n",
      "and improved report accuracy, leading to a more reliable information flow for decision-making \n",
      "processes. \n",
      "● Designed and implemented a user-centric facets UI on the category page, leveraging Solr for \n",
      "seamless data integration from the database, enhanced user satisfaction scores by 20% based \n",
      "on post-launch surveys. \n",
      " \n",
      "EDUCATION \n",
      " \n",
      "Bachelor in Computer Application Kumaun University, Nainital  \n",
      " May-2013 - Jun-2016 \n",
      " \n",
      "Certifications \n",
      " \n",
      "● SAP Certified Development Professional - SAP Commerce Cloud Developer  \n",
      "● Generative AI at SAP \n",
      " \n",
      "SKILLS \n",
      "● SAP Commerce \n",
      "● Punchout \n",
      "● Java/ Java EE  \n",
      "● MySQL / SQL/ Oracle \n",
      "● Spring \n",
      "● JavaScript \n",
      "● S4 integration using \n",
      "BTP. \n",
      "● Cloud Hot folder \n",
      "integration \n",
      "● Gen AI \n",
      "● JUnit \n",
      "● Web Services \n",
      "(Rest/Soap) \n",
      "● Docker \n",
      "● Splunk \n",
      "● Groovy \n",
      "● Git/Bitbucket \n",
      "● Ajax \n",
      "● Ant \n",
      "● Postman \n",
      "● Maven \n",
      "● SonarQube \n",
      "● Data structure and \n",
      "Algorithm \n",
      "● Microservices \n",
      "● Html \n",
      "● React\n",
      "\n",
      "Accenture, Gurugram, Haryana\n",
      "Software Engineer Apr 2019 – Jun 2020\n",
      "○ Engaged with clients to extract detailed requirements for the Order Reconciliation project, the resulting insights\n",
      "directly influenced the design phase, ensuring alignment with client expectations.\n",
      "○ Spearheaded the design and development of a comprehensive Order Reconciliation system through SAP Commerce,\n",
      "incorporating composite cron jobs that reduced order discrepancies by 40% and improved processing time by 25%.\n",
      "○ Analyzed error logs and identified root causes for recurring production failures, implemented target fixes that\n",
      "decreased error rates by 45% and improved system reliability for over 30000 daily active users.\n",
      "TCS, Mumbai\n",
      "Assistant Systems Engineer Feb 2017 – Apr 2019\n",
      "○ Designed an OCR framework, increasing text recognition accuracy by 60%.\n",
      "○ Developed UI components for Croma, including cart page, banners, and review integration.\n",
      "○ Implemented a streamlined PIM reporting Cron Job that reduced data processing time by 60% and improved\n",
      "report accuracy, leading to a more reliable information flow for decision-making processes.\n",
      "○ Designed and implemented a user-centric facets UI on the category page, leveraging Solr for seamless data\n",
      "integration from the database, enhanced user satisfaction scores by 20% based on post-launch surveys.\n",
      "Education\n",
      "Kumaun University, Nainital\n",
      "Bachelor in Computer Application 2013 – 2016\n",
      "Certifications\n",
      "SAP: SAP Certified Development Professional - SAP Commerce Cloud Developer\n",
      "AI: Generative AI at SAP\n",
      "2/2\n",
      "\n",
      "by15%withintheﬁrstquarterofimplementation.\n",
      "Accenture, Gurugram,Haryana Apr2019–Jun2020Software Engineer● Engagedwithclientsthrough12structuredmeetingstoextractdetailedrequirementsfortheOrderReconciliationproject,theresultinginsightsdirectlyinﬂuencedthedesignphase,ensuringalignmentwithclientexpectations.● SpearheadedthedesignanddevelopmentofacomprehensiveOrderReconciliationsystemthroughSAPCommerce,incorporatingcompositecronjobsthatreducedorderdiscrepanciesby40%andimprovedprocessingtimeby25%.● Analyzederrorlogsandidentiﬁedrootcausesforrecurringproductionfailures,implementedtargetﬁxesthatdecreasederrorratesby45%andimprovedsystemreliabilityforover30000dailyactiveusers.\n",
      "TCS, Mumbai Feb2017–Apr2019Assistant Systems Engineer● DesignedamachinelearningframeworkforOCRthatanalyzedandrecognizedtextfromdiverseformats,increasingrecognitionaccuracyby60%andreducingerrorratesindataentry,leadingtoimprovedoverallproductivity.● DevelopedthecustomizeUIandcomponentsforCroma(PLP,CartPage,RotatingBanners,HomePage,Headers,Shop-By-Category,PDPtechspeciﬁcation,3partyintegrationforratingandreviewsdata,CustomizedratingandreviewforCroma,Comparepageandotherstaticpages).● ImplementedastreamlinedPIMreportingCronJobthatreduceddataprocessingtimeby60%andimprovedreportaccuracy,leadingtoamorereliableinformationﬂowfordecision-makingprocesses.● Designedandimplementedauser-centricfacetsUIonthecategorypage,leveragingSolrforseamlessdataintegrationfromthedatabase,enhancedusersatisfactionscoresby20%basedonpost-launchsurveys.\n",
      "EDUCATION\n",
      "BachelorinComputerApplicationKumaunUniversity,Nainital\n",
      "May-2013-Jun-2016\n",
      "Certifications\n",
      "● SAPCertiﬁedDevelopmentProfessional-SAPCommerceCloudDeveloper● GenerativeAIatSAP\n",
      "SKILLS\n",
      "●Java/JavaEE●Spring●WebServices(Rest&Soap)●Microservices●Datastructure&Algorithms●Kafka\n",
      "●JavaScript●Ajax●React●NoSQL/Cassandra●SQL●MySQL●PostgreSQL\n",
      "●Kubernetes●Docker●GenAI●SAPCommerce●Solr●JUnit●Groovy\n",
      "●Git, Bitbucket●Swagger●Splunk, Kibana●AWS, GCP●GraphQL\n",
      "Human: Who is SAP certified?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: Mohan Salal is SAP certified as a SAP Certified Development Professional - SAP Commerce Cloud Developer.\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Who is SAP certified?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2136153b-d2f6-4c58-a0e3-78c3a932cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n",
      "Number of requested results 25 is greater than number of elements in index 11, updating n_results = 11\n"
     ]
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644753e7-17f3-4999-a37a-b6aebf1e4579",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Try applying this to your own folder of data, so that you create a personal knowledge worker, an expert on your own information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4745a-0a6c-4544-b78b-c827cfec1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
